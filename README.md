# RNN 
循环神经网络 (RNN, Recurrent Neural Network)

循环神经网络（RNN）是一类用于处理序列数据的神经网络，尤其擅长处理时间序列、语音识别、语言模型、文本生成等任务。与传统的前馈神经网络不同，RNN 具有内部的循环结构，这使得它可以在输入序列中的每个时间步保持“记忆”状态，并根据历史信息来影响当前的输出。
RNN 的工作原理

RNN 通过引入 隐藏状态（Hidden State）来存储序列中之前的时间步信息。每个时间步的数据都会影响到后续时间步的计算，形成了一个递归的循环。RNN 的一个基本单元在每个时间步都会接受以下输入：

    当前时间步的输入（例如时间序列中的第 t 个数据点）。
    上一个时间步的隐藏状态（来自 t-1 的隐藏状态）。

公式上，RNN 的每个时间步 t 的隐藏状态 htht​ 是由当前输入 xtxt​ 和前一个隐藏状态 ht−1ht−1​ 通过一个函数（通常是非线性激活函数，如 tanh）组合而成：

ht=f(Whht−1+Wxxt+bh)ht​=f(Wh​ht−1​+Wx​xt​+bh​)

其中：

    WhWh​ 是与隐藏状态相关的权重矩阵，
    WxWx​ 是与输入相关的权重矩阵，
    bhbh​ 是偏置项。

最终输出 ytyt​ 可以通过隐藏状态计算得出：

yt=f(Wyht+by)yt​=f(Wy​ht​+by​)
RNN 的特性

    时间依赖性：RNN 可以利用序列的上下文信息。网络的当前输出不仅依赖于当前输入，还依赖于之前的输入数据。它是顺序处理数据的网络。

    参数共享：RNN 在每个时间步共享相同的权重和偏置，这使得网络能够有效地处理任意长度的序列而无需增加额外的参数。

RNN 的局限性

尽管 RNN 在序列数据上取得了成功，但它存在以下局限性：

    长期依赖问题：RNN 在处理长序列时，信息逐渐在时间步之间“遗忘”，无法有效保留早期的输入信息。网络无法很好地记住远处的依赖关系，称之为 长期依赖问题。

    梯度消失和爆炸问题：在反向传播时，由于链式法则，梯度会逐渐衰减或膨胀，导致训练变得困难。这就是 梯度消失 或 梯度爆炸 的问题。

改进的 RNN：LSTM 和 GRU

为了克服 RNN 的长期依赖问题，出现了两种重要的改进版本：

    长短期记忆网络 (LSTM)： LSTM 引入了三个门机制：输入门、遗忘门和输出门，用来控制信息的流入、流出和保留。LSTM 能够记住长序列中的重要信息，并有效应对梯度消失问题。

    门控循环单元 (GRU)： GRU 是 LSTM 的简化版本，只有两个门（更新门和重置门）。它能够提供类似 LSTM 的效果，同时减少了计算复杂度。

RNN 的应用

    语言模型：RNN 可以用于生成序列，例如文本生成、机器翻译等。
    语音识别：RNN 可以从语音信号中学习时间序列特征，用于语音识别和分类。
    时间序列预测：在股票、天气等数据中，RNN 被广泛用于时间序列的预测任务。
    自然语言处理：RNN 是早期用于自然语言处理任务（如情感分析、命名实体识别、机器翻译等）的基础模型。

总结

RNN 通过其循环结构，使得它特别适用于处理具有时间依赖性的数据。但由于其长期依赖性的问题，LSTM 和 GRU 等变种模型逐渐取代了原始 RNN，成为了主流的解决方案。这些模型广泛应用于自然语言处理、语音识别、时间序列预测等领域。
RNN用于处理时间序列数据：在MLP的基础上，第一次处理的输出给了第二次处理

![image](https://github.com/user-attachments/assets/225d4ba8-614d-46e2-ac6f-a8119094b5ff)

**缺陷：** 在向后传播时，前面的信息逐渐被忽视，距离越远忘记越多。

![Screenshot from 2024-10-14 13-32-24](https://github.com/user-attachments/assets/087b88a0-3c37-4723-aa06-fad7523bd205)

# LSTM

增加记忆细胞，传递远处的重要信息。
![Screenshot from 2024-10-14 13-45-44](https://github.com/user-attachments/assets/0713c963-44d3-49ae-9afd-813914c5df79)

增加不同的门，实现记忆遗忘，解决了RNN的梯度消失的问题。

![Screenshot from 2024-10-14 13-55-30](https://github.com/user-attachments/assets/051439bc-f165-4087-a4b0-435c47cac143)
